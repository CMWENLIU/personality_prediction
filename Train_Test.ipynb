{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:-------------------------------------------------------\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=32\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=300\n",
      "EMBEDDING_DIR=./embedding/\n",
      "EVALUATE_EVERY=100\n",
      "EXTRA_DATA_DIR=./extra-data/\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LABELED_DATA_DIR=./labeled-data/\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=100\n",
      "NUM_FILTERS=64\n",
      "TRAIN_VEC=False\n",
      "TRAINED_EMBEDDING=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"labeled_data_dir\", \"./labeled-data/\", \"Data directory for labeled data.\")\n",
    "tf.flags.DEFINE_string(\"extra_data_dir\", \"./extra-data/\", \"Data directory for extra data.\")\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_boolean(\"train_vec\", False, \"Train vector using our data\")\n",
    "tf.flags.DEFINE_boolean(\"trained_embedding\", True, \"Allow trained embedding or random embedding\")\n",
    "tf.flags.DEFINE_string(\"embedding_dir\", \"./embedding/\", \"Data directory for trained embedding.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:-------------------------------------------------------\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continous to discrete data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train_data.csv')\n",
    "bins = [0, 3, 3.5, 4, 5]\n",
    "category = pd.cut(df.E_Scale_score,bins)\n",
    "category = category.to_frame()\n",
    "category.columns = ['range']\n",
    "#concatenate age and its bin\n",
    "df_new = pd.concat([df,category],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "    \n",
    "#draw histogram plot\n",
    "sns.countplot(x = 'range', data = df_new, palette = 'hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate text data\n",
    "for i in range(len(bins) - 1):\n",
    "    newdf = df[(bins[i] < df['E_Scale_score']) & (df['E_Scale_score'] <= bins[i+1])]\n",
    "    texts = newdf['open_ended_1']\n",
    "    file = os.path.join('labeled-data/', str(i) + '.txt')\n",
    "    with open(file, 'w') as f:\n",
    "        for item in texts:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for CNN classification model--------------------------\n",
      "Following is the data loaded for CNN model:\n",
      "./labeled-data/0.txt\n",
      "./labeled-data/1.txt\n",
      "./labeled-data/2.txt\n",
      "./labeled-data/3.txt\n",
      "Following is the labels for supervised learning:\n",
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "[0, 0, 1, 0]\n",
      "[0, 0, 0, 1]\n",
      "Data is successfully loaded!\n",
      "Vocabulary Size: 2240\n",
      "Train/Dev split: 740/185\n",
      "Shape of x_train:  (740, 202)\n",
      "Shape of y_train:  (740, 4)\n",
      "Shape of x_dev:  (185, 202)\n",
      "Shape of y_dev:  (185, 4)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data for CNN classification model--------------------------\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.labeled_data_dir)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "#max_document_length = max([len(x) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "print(\"Data is successfully loaded!\")\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "print('Shape of x_train: ', x_train.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of x_dev: ', x_dev.shape)\n",
    "print('Shape of y_dev: ', y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/bear/personality_prediction/runs/1556742642\n",
      "\n",
      "Load embedding file--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if (FLAGS.trained_embedding):\n",
    "            vocabulary = vocab_processor.vocabulary_\n",
    "            initW = None\n",
    "            print(\"Load embedding file--------------------------------------------------\")\n",
    "            #yes_transfer, no_transfer = 'all', 'labeled'\n",
    "            #l_embfile = no_transfer + '.' + str(FLAGS.embedding_dim) + '.vec'\n",
    "            al_embfile = 'all.' + str(FLAGS.embedding_dim) + '.vec'\n",
    "            if FLAGS.train_vec:\n",
    "                datafolders = [FLAGS.labeled_data_dir, FLAGS.extra_data_dir]\n",
    "                if os.path.isfile(FLAGS.embedding_dir + al_embfile):\n",
    "                    print(al_embfile + 'has already been exist')\n",
    "                else:\n",
    "                    data_helpers.train_word_embedding(FLAGS.embedding_dim, \"all\", df)\n",
    "                embfile = FLAGS.embedding_dir + al_embfile\n",
    "            else:\n",
    "                embfile = \"/home/bear/Downloads/glove.42B.300d.txt\"\n",
    "            initW = data_helpers.load_embedding_vectors(vocabulary, embfile, FLAGS.embedding_dim)\n",
    "            print('embedding file: ' + embfile + ' has been sucessfully loaded!\\n')\n",
    "            sess.run(cnn.W.assign(initW))\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
