{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:-------------------------------------------------------\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=32\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.2\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=300\n",
      "EMBEDDING_DIR=./embedding/\n",
      "EVALUATE_EVERY=100\n",
      "EXTRA_DATA_DIR=./extra-data/\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LABELED_DATA_DIR=./labeled-data/\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=100\n",
      "NUM_FILTERS=64\n",
      "TRAIN_VEC=False\n",
      "TRAINED_EMBEDDING=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .2, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"labeled_data_dir\", \"./labeled-data/\", \"Data directory for labeled data.\")\n",
    "tf.flags.DEFINE_string(\"extra_data_dir\", \"./extra-data/\", \"Data directory for extra data.\")\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_boolean(\"train_vec\", False, \"Train vector using our data\")\n",
    "tf.flags.DEFINE_boolean(\"trained_embedding\", True, \"Allow trained embedding or random embedding\")\n",
    "tf.flags.DEFINE_string(\"embedding_dir\", \"./embedding/\", \"Data directory for trained embedding.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:-------------------------------------------------------\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continous to discrete data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train_data.csv')\n",
    "bins = [0, 2.5, 3, 3.25, 3.5, 3.75, 4, 4.5, 5]\n",
    "category = pd.cut(df.E_Scale_score,bins)\n",
    "category = category.to_frame()\n",
    "category.columns = ['range']\n",
    "#concatenate age and its bin\n",
    "df_new = pd.concat([df,category],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "    \n",
    "#draw histogram plot\n",
    "sns.countplot(x = 'range', data = df_new, palette = 'hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate text data\n",
    "for i in range(len(bins) - 1):\n",
    "    newdf = df[(bins[i] < df['E_Scale_score']) & (df['E_Scale_score'] <= bins[i+1])]\n",
    "    texts = newdf['open_ended_1']\n",
    "    file = os.path.join('labeled-data/', str(i) + '.txt')\n",
    "    with open(file, 'w') as f:\n",
    "        for item in texts:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for CNN classification model--------------------------\n",
      "Following is the data loaded for CNN model:\n",
      "./labeled-data/0.txt\n",
      "./labeled-data/1.txt\n",
      "./labeled-data/2.txt\n",
      "./labeled-data/3.txt\n",
      "./labeled-data/4.txt\n",
      "./labeled-data/5.txt\n",
      "./labeled-data/6.txt\n",
      "./labeled-data/7.txt\n",
      "Following is the labels for supervised learning:\n",
      "[1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Data is successfully loaded!\n",
      "Vocabulary Size: 2240\n",
      "Train/Dev split: 740/185\n",
      "Shape of x_train:  (740, 202)\n",
      "Shape of y_train:  (740, 8)\n",
      "Shape of x_dev:  (185, 202)\n",
      "Shape of y_dev:  (185, 8)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data for CNN classification model--------------------------\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.labeled_data_dir)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "#max_document_length = max([len(x) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "print(\"Data is successfully loaded!\")\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "print('Shape of x_train: ', x_train.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of x_dev: ', x_dev.shape)\n",
    "print('Shape of y_dev: ', y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/bear/personality_prediction/runs/1556726532\n",
      "\n",
      "Load embedding file--------------------------------------------------\n",
      "embedding file: /home/bear/Downloads/glove.42B.300d.txt has been sucessfully loaded!\n",
      "\n",
      "2019-05-01T11:07:53.956115: step 1, loss 9.26341, acc 0.25\n",
      "2019-05-01T11:07:55.472844: step 2, loss 8.39892, acc 0.15625\n",
      "2019-05-01T11:07:56.882362: step 3, loss 5.11409, acc 0.21875\n",
      "2019-05-01T11:07:58.244504: step 4, loss 4.64168, acc 0.1875\n",
      "2019-05-01T11:07:59.510841: step 5, loss 5.50297, acc 0.03125\n",
      "2019-05-01T11:08:00.725067: step 6, loss 4.54011, acc 0.15625\n",
      "2019-05-01T11:08:02.134072: step 7, loss 4.64269, acc 0.125\n",
      "2019-05-01T11:08:03.447770: step 8, loss 4.64939, acc 0.1875\n",
      "2019-05-01T11:08:04.568610: step 9, loss 5.58755, acc 0.15625\n",
      "2019-05-01T11:08:05.655229: step 10, loss 5.62425, acc 0.15625\n",
      "2019-05-01T11:08:06.759539: step 11, loss 5.15751, acc 0.1875\n",
      "2019-05-01T11:08:08.088518: step 12, loss 4.96166, acc 0.03125\n",
      "2019-05-01T11:08:08.701035: step 13, loss 4.72213, acc 0.1875\n",
      "2019-05-01T11:08:09.344404: step 14, loss 4.81785, acc 0.125\n",
      "2019-05-01T11:08:09.880190: step 15, loss 5.59929, acc 0.1875\n",
      "2019-05-01T11:08:10.416941: step 16, loss 4.25032, acc 0.15625\n",
      "2019-05-01T11:08:10.972984: step 17, loss 4.79555, acc 0.125\n",
      "2019-05-01T11:08:11.612147: step 18, loss 5.03715, acc 0.21875\n",
      "2019-05-01T11:08:12.022610: step 19, loss 3.35282, acc 0.125\n",
      "2019-05-01T11:08:12.450447: step 20, loss 5.21438, acc 0.0625\n",
      "2019-05-01T11:08:13.022319: step 21, loss 5.26979, acc 0.0625\n",
      "2019-05-01T11:08:14.120257: step 22, loss 4.10168, acc 0.09375\n",
      "2019-05-01T11:08:14.864735: step 23, loss 3.73047, acc 0.21875\n",
      "2019-05-01T11:08:15.160819: step 24, loss 6.99764, acc 0\n",
      "2019-05-01T11:08:16.120997: step 25, loss 3.06814, acc 0.28125\n",
      "2019-05-01T11:08:16.679366: step 26, loss 4.11794, acc 0.1875\n",
      "2019-05-01T11:08:17.206676: step 27, loss 3.72593, acc 0.0625\n",
      "2019-05-01T11:08:17.941690: step 28, loss 3.8991, acc 0.15625\n",
      "2019-05-01T11:08:18.667746: step 29, loss 3.21787, acc 0.3125\n",
      "2019-05-01T11:08:19.106309: step 30, loss 3.06845, acc 0.21875\n",
      "2019-05-01T11:08:19.640121: step 31, loss 3.71167, acc 0.1875\n",
      "2019-05-01T11:08:20.039223: step 32, loss 4.25335, acc 0.125\n",
      "2019-05-01T11:08:20.576867: step 33, loss 3.47209, acc 0.28125\n",
      "2019-05-01T11:08:21.201686: step 34, loss 3.09422, acc 0.25\n",
      "2019-05-01T11:08:21.750811: step 35, loss 3.17593, acc 0.25\n",
      "2019-05-01T11:08:22.171647: step 36, loss 4.14746, acc 0.125\n",
      "2019-05-01T11:08:22.562420: step 37, loss 4.17446, acc 0.125\n",
      "2019-05-01T11:08:22.956402: step 38, loss 2.58434, acc 0.375\n",
      "2019-05-01T11:08:23.359396: step 39, loss 2.82354, acc 0.1875\n",
      "2019-05-01T11:08:23.771860: step 40, loss 2.69637, acc 0.28125\n",
      "2019-05-01T11:08:24.181335: step 41, loss 3.14077, acc 0.15625\n",
      "2019-05-01T11:08:24.597449: step 42, loss 3.54506, acc 0.0625\n",
      "2019-05-01T11:08:25.014326: step 43, loss 3.09868, acc 0.15625\n",
      "2019-05-01T11:08:25.427208: step 44, loss 3.42189, acc 0.09375\n",
      "2019-05-01T11:08:25.863658: step 45, loss 3.51756, acc 0.125\n",
      "2019-05-01T11:08:26.284655: step 46, loss 3.05403, acc 0.15625\n",
      "2019-05-01T11:08:26.687026: step 47, loss 2.75046, acc 0.1875\n",
      "2019-05-01T11:08:26.757530: step 48, loss 2.17696, acc 0.25\n",
      "2019-05-01T11:08:27.208747: step 49, loss 3.06378, acc 0.15625\n",
      "2019-05-01T11:08:27.612978: step 50, loss 2.45107, acc 0.21875\n",
      "2019-05-01T11:08:28.019076: step 51, loss 2.51049, acc 0.3125\n",
      "2019-05-01T11:08:28.452232: step 52, loss 3.27688, acc 0.125\n",
      "2019-05-01T11:08:28.861174: step 53, loss 2.62283, acc 0.21875\n",
      "2019-05-01T11:08:29.292396: step 54, loss 2.32963, acc 0.25\n",
      "2019-05-01T11:08:29.704518: step 55, loss 2.49043, acc 0.28125\n",
      "2019-05-01T11:08:30.148260: step 56, loss 2.48161, acc 0.1875\n",
      "2019-05-01T11:08:30.563201: step 57, loss 3.50287, acc 0.15625\n",
      "2019-05-01T11:08:30.958017: step 58, loss 2.65306, acc 0.21875\n",
      "2019-05-01T11:08:31.368452: step 59, loss 2.70059, acc 0.28125\n",
      "2019-05-01T11:08:31.763691: step 60, loss 3.06853, acc 0.21875\n",
      "2019-05-01T11:08:32.251380: step 61, loss 2.78207, acc 0.25\n",
      "2019-05-01T11:08:32.823466: step 62, loss 1.92832, acc 0.34375\n",
      "2019-05-01T11:08:33.248691: step 63, loss 2.42249, acc 0.3125\n",
      "2019-05-01T11:08:33.833135: step 64, loss 2.59766, acc 0.21875\n",
      "2019-05-01T11:08:34.533460: step 65, loss 2.77152, acc 0.21875\n",
      "2019-05-01T11:08:35.218183: step 66, loss 2.06713, acc 0.4375\n",
      "2019-05-01T11:08:35.842811: step 67, loss 2.57247, acc 0.15625\n",
      "2019-05-01T11:08:36.258392: step 68, loss 1.99692, acc 0.25\n",
      "2019-05-01T11:08:36.720568: step 69, loss 2.89926, acc 0.125\n",
      "2019-05-01T11:08:37.271930: step 70, loss 2.49335, acc 0.25\n",
      "2019-05-01T11:08:37.829385: step 71, loss 1.9384, acc 0.34375\n",
      "2019-05-01T11:08:37.951536: step 72, loss 2.33776, acc 0.25\n",
      "2019-05-01T11:08:38.490995: step 73, loss 2.0177, acc 0.1875\n",
      "2019-05-01T11:08:38.892798: step 74, loss 1.67255, acc 0.3125\n",
      "2019-05-01T11:08:39.303062: step 75, loss 1.59474, acc 0.40625\n",
      "2019-05-01T11:08:39.857402: step 76, loss 1.76716, acc 0.4375\n",
      "2019-05-01T11:08:40.316407: step 77, loss 2.43472, acc 0.25\n",
      "2019-05-01T11:08:40.762289: step 78, loss 2.38244, acc 0.25\n",
      "2019-05-01T11:08:41.176927: step 79, loss 1.96041, acc 0.34375\n",
      "2019-05-01T11:08:41.607361: step 80, loss 2.33478, acc 0.15625\n",
      "2019-05-01T11:08:42.006219: step 81, loss 1.66834, acc 0.46875\n",
      "2019-05-01T11:08:42.608488: step 82, loss 1.94616, acc 0.34375\n",
      "2019-05-01T11:08:43.054773: step 83, loss 2.17655, acc 0.3125\n",
      "2019-05-01T11:08:43.494147: step 84, loss 1.9949, acc 0.34375\n",
      "2019-05-01T11:08:43.891664: step 85, loss 1.97508, acc 0.3125\n",
      "2019-05-01T11:08:44.312431: step 86, loss 1.66223, acc 0.46875\n",
      "2019-05-01T11:08:44.908356: step 87, loss 1.50029, acc 0.5\n",
      "2019-05-01T11:08:45.347702: step 88, loss 2.11233, acc 0.21875\n",
      "2019-05-01T11:08:45.740818: step 89, loss 2.09555, acc 0.34375\n",
      "2019-05-01T11:08:46.150737: step 90, loss 1.83194, acc 0.40625\n",
      "2019-05-01T11:08:46.568628: step 91, loss 2.39585, acc 0.21875\n",
      "2019-05-01T11:08:46.982907: step 92, loss 2.1918, acc 0.25\n",
      "2019-05-01T11:08:47.552421: step 93, loss 1.83173, acc 0.40625\n",
      "2019-05-01T11:08:48.095805: step 94, loss 1.69966, acc 0.28125\n",
      "2019-05-01T11:08:48.509942: step 95, loss 1.69764, acc 0.46875\n",
      "2019-05-01T11:08:48.581354: step 96, loss 1.31833, acc 0.5\n",
      "2019-05-01T11:08:48.979411: step 97, loss 1.42989, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:08:49.391895: step 98, loss 1.72832, acc 0.375\n",
      "2019-05-01T11:08:49.811116: step 99, loss 1.76855, acc 0.34375\n",
      "2019-05-01T11:08:50.285655: step 100, loss 1.78529, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:08:51.152791: step 100, loss 2.22419, acc 0.156757\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-100\n",
      "\n",
      "2019-05-01T11:08:51.673196: step 101, loss 1.67703, acc 0.4375\n",
      "2019-05-01T11:08:52.126851: step 102, loss 1.62842, acc 0.46875\n",
      "2019-05-01T11:08:52.595698: step 103, loss 1.52032, acc 0.46875\n",
      "2019-05-01T11:08:53.060450: step 104, loss 1.40545, acc 0.4375\n",
      "2019-05-01T11:08:53.510329: step 105, loss 1.93734, acc 0.25\n",
      "2019-05-01T11:08:53.994994: step 106, loss 1.71445, acc 0.4375\n",
      "2019-05-01T11:08:54.430710: step 107, loss 1.5824, acc 0.40625\n",
      "2019-05-01T11:08:54.883763: step 108, loss 1.52091, acc 0.46875\n",
      "2019-05-01T11:08:55.336247: step 109, loss 1.47768, acc 0.375\n",
      "2019-05-01T11:08:55.768288: step 110, loss 1.28335, acc 0.5625\n",
      "2019-05-01T11:08:56.236454: step 111, loss 1.7032, acc 0.46875\n",
      "2019-05-01T11:08:56.681211: step 112, loss 1.58754, acc 0.46875\n",
      "2019-05-01T11:08:57.124137: step 113, loss 1.45518, acc 0.4375\n",
      "2019-05-01T11:08:57.557373: step 114, loss 1.45003, acc 0.5\n",
      "2019-05-01T11:08:57.985604: step 115, loss 1.80936, acc 0.34375\n",
      "2019-05-01T11:08:58.432677: step 116, loss 1.41392, acc 0.53125\n",
      "2019-05-01T11:08:59.007398: step 117, loss 1.64333, acc 0.375\n",
      "2019-05-01T11:08:59.559247: step 118, loss 1.69395, acc 0.46875\n",
      "2019-05-01T11:09:00.078195: step 119, loss 1.26756, acc 0.53125\n",
      "2019-05-01T11:09:00.172384: step 120, loss 1.11057, acc 0.75\n",
      "2019-05-01T11:09:00.628103: step 121, loss 1.34893, acc 0.53125\n",
      "2019-05-01T11:09:01.067976: step 122, loss 1.26943, acc 0.46875\n",
      "2019-05-01T11:09:01.528611: step 123, loss 1.47747, acc 0.5625\n",
      "2019-05-01T11:09:01.971729: step 124, loss 1.19643, acc 0.53125\n",
      "2019-05-01T11:09:02.436353: step 125, loss 1.11504, acc 0.5\n",
      "2019-05-01T11:09:02.877941: step 126, loss 1.24308, acc 0.59375\n",
      "2019-05-01T11:09:03.357873: step 127, loss 1.05095, acc 0.65625\n",
      "2019-05-01T11:09:03.796662: step 128, loss 1.24214, acc 0.625\n",
      "2019-05-01T11:09:04.246564: step 129, loss 1.45427, acc 0.53125\n",
      "2019-05-01T11:09:04.692728: step 130, loss 1.22399, acc 0.53125\n",
      "2019-05-01T11:09:05.160804: step 131, loss 1.28988, acc 0.625\n",
      "2019-05-01T11:09:05.598308: step 132, loss 1.59778, acc 0.375\n",
      "2019-05-01T11:09:06.031276: step 133, loss 0.89481, acc 0.75\n",
      "2019-05-01T11:09:06.519299: step 134, loss 1.05113, acc 0.65625\n",
      "2019-05-01T11:09:06.951695: step 135, loss 1.24947, acc 0.53125\n",
      "2019-05-01T11:09:07.452460: step 136, loss 1.21666, acc 0.53125\n",
      "2019-05-01T11:09:08.045341: step 137, loss 0.951185, acc 0.75\n",
      "2019-05-01T11:09:08.490170: step 138, loss 1.19971, acc 0.4375\n",
      "2019-05-01T11:09:08.919343: step 139, loss 1.37359, acc 0.5\n",
      "2019-05-01T11:09:09.385555: step 140, loss 1.18634, acc 0.625\n",
      "2019-05-01T11:09:09.803376: step 141, loss 1.54708, acc 0.5\n",
      "2019-05-01T11:09:10.263611: step 142, loss 1.07653, acc 0.625\n",
      "2019-05-01T11:09:10.762806: step 143, loss 1.22549, acc 0.65625\n",
      "2019-05-01T11:09:10.873443: step 144, loss 0.750105, acc 0.75\n",
      "2019-05-01T11:09:11.315323: step 145, loss 1.37458, acc 0.5\n",
      "2019-05-01T11:09:11.783526: step 146, loss 1.26818, acc 0.5625\n",
      "2019-05-01T11:09:12.326102: step 147, loss 1.03692, acc 0.6875\n",
      "2019-05-01T11:09:12.839765: step 148, loss 1.06834, acc 0.625\n",
      "2019-05-01T11:09:13.484218: step 149, loss 0.940285, acc 0.6875\n",
      "2019-05-01T11:09:13.967375: step 150, loss 0.936645, acc 0.78125\n",
      "2019-05-01T11:09:14.559962: step 151, loss 1.12495, acc 0.65625\n",
      "2019-05-01T11:09:15.047828: step 152, loss 1.13298, acc 0.5625\n",
      "2019-05-01T11:09:15.555284: step 153, loss 1.16318, acc 0.59375\n",
      "2019-05-01T11:09:16.086238: step 154, loss 1.04689, acc 0.59375\n",
      "2019-05-01T11:09:16.570596: step 155, loss 0.975563, acc 0.71875\n",
      "2019-05-01T11:09:17.208365: step 156, loss 0.931891, acc 0.75\n",
      "2019-05-01T11:09:17.792642: step 157, loss 0.982258, acc 0.6875\n",
      "2019-05-01T11:09:18.467070: step 158, loss 0.91309, acc 0.75\n",
      "2019-05-01T11:09:19.173559: step 159, loss 1.21513, acc 0.5\n",
      "2019-05-01T11:09:19.820558: step 160, loss 0.879662, acc 0.65625\n",
      "2019-05-01T11:09:20.626208: step 161, loss 0.953152, acc 0.71875\n",
      "2019-05-01T11:09:21.421484: step 162, loss 1.02644, acc 0.6875\n",
      "2019-05-01T11:09:22.067381: step 163, loss 1.00541, acc 0.65625\n",
      "2019-05-01T11:09:22.668311: step 164, loss 0.928163, acc 0.75\n",
      "2019-05-01T11:09:23.269393: step 165, loss 1.13956, acc 0.59375\n",
      "2019-05-01T11:09:23.744494: step 166, loss 0.928796, acc 0.71875\n",
      "2019-05-01T11:09:24.451710: step 167, loss 0.950134, acc 0.8125\n",
      "2019-05-01T11:09:24.552399: step 168, loss 0.719846, acc 0.75\n",
      "2019-05-01T11:09:25.003275: step 169, loss 0.792516, acc 0.75\n",
      "2019-05-01T11:09:25.488147: step 170, loss 0.930701, acc 0.75\n",
      "2019-05-01T11:09:25.947943: step 171, loss 1.00084, acc 0.71875\n",
      "2019-05-01T11:09:26.595922: step 172, loss 0.842189, acc 0.8125\n",
      "2019-05-01T11:09:27.050302: step 173, loss 0.875609, acc 0.875\n",
      "2019-05-01T11:09:27.534739: step 174, loss 0.857726, acc 0.75\n",
      "2019-05-01T11:09:28.046364: step 175, loss 0.861773, acc 0.84375\n",
      "2019-05-01T11:09:28.538180: step 176, loss 0.744366, acc 0.78125\n",
      "2019-05-01T11:09:28.996435: step 177, loss 0.959615, acc 0.75\n",
      "2019-05-01T11:09:29.530717: step 178, loss 0.995469, acc 0.71875\n",
      "2019-05-01T11:09:29.946470: step 179, loss 1.01326, acc 0.78125\n",
      "2019-05-01T11:09:30.389168: step 180, loss 0.812152, acc 0.8125\n",
      "2019-05-01T11:09:30.807570: step 181, loss 0.876289, acc 0.59375\n",
      "2019-05-01T11:09:31.266444: step 182, loss 1.01118, acc 0.71875\n",
      "2019-05-01T11:09:31.741021: step 183, loss 0.733286, acc 0.90625\n",
      "2019-05-01T11:09:32.189915: step 184, loss 0.881913, acc 0.6875\n",
      "2019-05-01T11:09:32.824114: step 185, loss 0.952326, acc 0.75\n",
      "2019-05-01T11:09:33.376125: step 186, loss 0.882104, acc 0.78125\n",
      "2019-05-01T11:09:33.995889: step 187, loss 0.886247, acc 0.75\n",
      "2019-05-01T11:09:34.495198: step 188, loss 1.00532, acc 0.75\n",
      "2019-05-01T11:09:34.915921: step 189, loss 0.964051, acc 0.71875\n",
      "2019-05-01T11:09:35.330238: step 190, loss 0.83351, acc 0.78125\n",
      "2019-05-01T11:09:35.735456: step 191, loss 0.886632, acc 0.78125\n",
      "2019-05-01T11:09:35.805521: step 192, loss 0.76962, acc 1\n",
      "2019-05-01T11:09:36.266861: step 193, loss 0.755879, acc 0.78125\n",
      "2019-05-01T11:09:36.745193: step 194, loss 0.741082, acc 0.75\n",
      "2019-05-01T11:09:37.223993: step 195, loss 0.709851, acc 0.90625\n",
      "2019-05-01T11:09:37.675863: step 196, loss 0.929394, acc 0.71875\n",
      "2019-05-01T11:09:38.148870: step 197, loss 0.754978, acc 0.84375\n",
      "2019-05-01T11:09:38.692007: step 198, loss 0.722575, acc 0.75\n",
      "2019-05-01T11:09:39.283605: step 199, loss 1.07882, acc 0.6875\n",
      "2019-05-01T11:09:39.791433: step 200, loss 0.720925, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:09:40.854593: step 200, loss 2.24285, acc 0.156757\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-200\n",
      "\n",
      "2019-05-01T11:09:41.405868: step 201, loss 0.614816, acc 0.875\n",
      "2019-05-01T11:09:41.801067: step 202, loss 0.728197, acc 0.84375\n",
      "2019-05-01T11:09:42.303806: step 203, loss 0.821915, acc 0.875\n",
      "2019-05-01T11:09:42.705737: step 204, loss 0.697422, acc 0.9375\n",
      "2019-05-01T11:09:43.242346: step 205, loss 0.785391, acc 0.75\n",
      "2019-05-01T11:09:43.726057: step 206, loss 0.821791, acc 0.8125\n",
      "2019-05-01T11:09:44.254128: step 207, loss 0.814092, acc 0.84375\n",
      "2019-05-01T11:09:44.997053: step 208, loss 0.565845, acc 0.90625\n",
      "2019-05-01T11:09:45.606384: step 209, loss 0.885526, acc 0.75\n",
      "2019-05-01T11:09:46.134009: step 210, loss 0.558508, acc 0.90625\n",
      "2019-05-01T11:09:46.635709: step 211, loss 0.704377, acc 0.875\n",
      "2019-05-01T11:09:47.088639: step 212, loss 0.802275, acc 0.84375\n",
      "2019-05-01T11:09:47.537606: step 213, loss 0.908725, acc 0.8125\n",
      "2019-05-01T11:09:47.950616: step 214, loss 0.833429, acc 0.78125\n",
      "2019-05-01T11:09:48.378398: step 215, loss 0.818874, acc 0.78125\n",
      "2019-05-01T11:09:48.466105: step 216, loss 0.542414, acc 1\n",
      "2019-05-01T11:09:49.005686: step 217, loss 0.561942, acc 0.90625\n",
      "2019-05-01T11:09:49.613940: step 218, loss 0.674823, acc 0.84375\n",
      "2019-05-01T11:09:50.171420: step 219, loss 0.961355, acc 0.71875\n",
      "2019-05-01T11:09:50.582885: step 220, loss 0.71454, acc 0.78125\n",
      "2019-05-01T11:09:50.979171: step 221, loss 0.701547, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:09:51.412092: step 222, loss 0.506794, acc 0.90625\n",
      "2019-05-01T11:09:51.807739: step 223, loss 0.6206, acc 0.96875\n",
      "2019-05-01T11:09:52.248019: step 224, loss 0.69092, acc 0.90625\n",
      "2019-05-01T11:09:52.647012: step 225, loss 0.661764, acc 0.8125\n",
      "2019-05-01T11:09:53.079640: step 226, loss 0.566012, acc 0.8125\n",
      "2019-05-01T11:09:53.537171: step 227, loss 0.652841, acc 0.84375\n",
      "2019-05-01T11:09:53.998136: step 228, loss 0.583826, acc 0.9375\n",
      "2019-05-01T11:09:54.547514: step 229, loss 0.65992, acc 0.90625\n",
      "2019-05-01T11:09:55.047091: step 230, loss 0.550623, acc 0.96875\n",
      "2019-05-01T11:09:55.671076: step 231, loss 0.530472, acc 0.9375\n",
      "2019-05-01T11:09:56.220332: step 232, loss 0.719498, acc 0.875\n",
      "2019-05-01T11:09:56.749465: step 233, loss 0.694765, acc 0.8125\n",
      "2019-05-01T11:09:57.425125: step 234, loss 0.653814, acc 0.9375\n",
      "2019-05-01T11:09:58.147338: step 235, loss 0.580975, acc 0.90625\n",
      "2019-05-01T11:09:58.792162: step 236, loss 0.630824, acc 0.875\n",
      "2019-05-01T11:09:59.251987: step 237, loss 0.586666, acc 0.90625\n",
      "2019-05-01T11:09:59.654073: step 238, loss 0.637349, acc 0.875\n",
      "2019-05-01T11:10:00.075397: step 239, loss 0.658348, acc 0.8125\n",
      "2019-05-01T11:10:00.176817: step 240, loss 0.423021, acc 1\n",
      "2019-05-01T11:10:00.766376: step 241, loss 0.543939, acc 0.9375\n",
      "2019-05-01T11:10:01.244461: step 242, loss 0.517112, acc 0.90625\n",
      "2019-05-01T11:10:01.870829: step 243, loss 0.549143, acc 0.9375\n",
      "2019-05-01T11:10:02.302673: step 244, loss 0.55677, acc 0.8125\n",
      "2019-05-01T11:10:02.695902: step 245, loss 0.559489, acc 0.9375\n",
      "2019-05-01T11:10:03.186828: step 246, loss 0.495024, acc 0.9375\n",
      "2019-05-01T11:10:03.741058: step 247, loss 0.479964, acc 0.90625\n",
      "2019-05-01T11:10:04.175495: step 248, loss 0.643466, acc 0.84375\n",
      "2019-05-01T11:10:04.589613: step 249, loss 0.520619, acc 0.96875\n",
      "2019-05-01T11:10:04.983302: step 250, loss 0.483152, acc 1\n",
      "2019-05-01T11:10:05.410429: step 251, loss 0.595421, acc 0.875\n",
      "2019-05-01T11:10:05.810792: step 252, loss 0.568106, acc 0.9375\n",
      "2019-05-01T11:10:06.263788: step 253, loss 0.576638, acc 0.90625\n",
      "2019-05-01T11:10:06.741473: step 254, loss 0.790113, acc 0.78125\n",
      "2019-05-01T11:10:07.428118: step 255, loss 0.800479, acc 0.8125\n",
      "2019-05-01T11:10:07.891879: step 256, loss 0.495131, acc 0.9375\n",
      "2019-05-01T11:10:08.791483: step 257, loss 0.721807, acc 0.84375\n",
      "2019-05-01T11:10:09.313074: step 258, loss 0.605696, acc 0.84375\n",
      "2019-05-01T11:10:09.998892: step 259, loss 0.516938, acc 0.90625\n",
      "2019-05-01T11:10:10.469638: step 260, loss 0.520528, acc 0.9375\n",
      "2019-05-01T11:10:10.952154: step 261, loss 0.705808, acc 0.8125\n",
      "2019-05-01T11:10:11.465858: step 262, loss 0.458814, acc 0.90625\n",
      "2019-05-01T11:10:11.986377: step 263, loss 0.540478, acc 0.8125\n",
      "2019-05-01T11:10:12.061043: step 264, loss 0.633935, acc 1\n",
      "2019-05-01T11:10:12.477757: step 265, loss 0.515354, acc 0.90625\n",
      "2019-05-01T11:10:12.909949: step 266, loss 0.599195, acc 0.84375\n",
      "2019-05-01T11:10:13.425378: step 267, loss 0.355389, acc 1\n",
      "2019-05-01T11:10:13.821088: step 268, loss 0.549845, acc 0.90625\n",
      "2019-05-01T11:10:14.284498: step 269, loss 0.560551, acc 0.90625\n",
      "2019-05-01T11:10:14.677720: step 270, loss 0.444153, acc 0.90625\n",
      "2019-05-01T11:10:15.093698: step 271, loss 0.547089, acc 0.9375\n",
      "2019-05-01T11:10:15.529813: step 272, loss 0.484842, acc 0.9375\n",
      "2019-05-01T11:10:15.927871: step 273, loss 0.584845, acc 0.875\n",
      "2019-05-01T11:10:16.361986: step 274, loss 0.606578, acc 0.875\n",
      "2019-05-01T11:10:16.746931: step 275, loss 0.429528, acc 0.96875\n",
      "2019-05-01T11:10:17.156838: step 276, loss 0.445448, acc 0.90625\n",
      "2019-05-01T11:10:17.575590: step 277, loss 0.56888, acc 0.875\n",
      "2019-05-01T11:10:17.978648: step 278, loss 0.356181, acc 1\n",
      "2019-05-01T11:10:18.421178: step 279, loss 0.577269, acc 0.90625\n",
      "2019-05-01T11:10:18.814150: step 280, loss 0.633329, acc 0.84375\n",
      "2019-05-01T11:10:19.274339: step 281, loss 0.486148, acc 0.9375\n",
      "2019-05-01T11:10:19.763937: step 282, loss 0.593353, acc 0.90625\n",
      "2019-05-01T11:10:20.298598: step 283, loss 0.421065, acc 0.9375\n",
      "2019-05-01T11:10:20.789341: step 284, loss 0.373844, acc 0.96875\n",
      "2019-05-01T11:10:21.312307: step 285, loss 0.599405, acc 0.8125\n",
      "2019-05-01T11:10:21.735789: step 286, loss 0.603785, acc 0.9375\n",
      "2019-05-01T11:10:22.240407: step 287, loss 0.546232, acc 0.9375\n",
      "2019-05-01T11:10:22.353793: step 288, loss 0.802953, acc 0.75\n",
      "2019-05-01T11:10:22.880340: step 289, loss 0.614721, acc 0.875\n",
      "2019-05-01T11:10:23.487465: step 290, loss 0.463055, acc 0.9375\n",
      "2019-05-01T11:10:23.932742: step 291, loss 0.401267, acc 0.96875\n",
      "2019-05-01T11:10:24.403839: step 292, loss 0.520686, acc 0.875\n",
      "2019-05-01T11:10:24.825213: step 293, loss 0.413657, acc 0.90625\n",
      "2019-05-01T11:10:25.276719: step 294, loss 0.492732, acc 0.96875\n",
      "2019-05-01T11:10:25.701178: step 295, loss 0.542662, acc 0.9375\n",
      "2019-05-01T11:10:26.165003: step 296, loss 0.407689, acc 0.9375\n",
      "2019-05-01T11:10:26.663551: step 297, loss 0.511653, acc 0.90625\n",
      "2019-05-01T11:10:27.123164: step 298, loss 0.498393, acc 0.96875\n",
      "2019-05-01T11:10:27.666238: step 299, loss 0.374862, acc 0.96875\n",
      "2019-05-01T11:10:28.139373: step 300, loss 0.635393, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:10:28.980776: step 300, loss 2.28348, acc 0.162162\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-300\n",
      "\n",
      "2019-05-01T11:10:29.564736: step 301, loss 0.430888, acc 0.90625\n",
      "2019-05-01T11:10:30.083267: step 302, loss 0.432595, acc 0.9375\n",
      "2019-05-01T11:10:30.556976: step 303, loss 0.45737, acc 0.96875\n",
      "2019-05-01T11:10:31.022924: step 304, loss 0.450713, acc 0.90625\n",
      "2019-05-01T11:10:31.523960: step 305, loss 0.446095, acc 0.90625\n",
      "2019-05-01T11:10:31.966728: step 306, loss 0.373646, acc 0.96875\n",
      "2019-05-01T11:10:32.423066: step 307, loss 0.380811, acc 0.96875\n",
      "2019-05-01T11:10:32.897405: step 308, loss 0.533979, acc 0.90625\n",
      "2019-05-01T11:10:33.397334: step 309, loss 0.424741, acc 0.96875\n",
      "2019-05-01T11:10:33.832876: step 310, loss 0.439294, acc 0.90625\n",
      "2019-05-01T11:10:34.398420: step 311, loss 0.46768, acc 0.90625\n",
      "2019-05-01T11:10:34.531405: step 312, loss 0.746755, acc 1\n",
      "2019-05-01T11:10:35.156180: step 313, loss 0.406525, acc 0.9375\n",
      "2019-05-01T11:10:35.619863: step 314, loss 0.425399, acc 0.96875\n",
      "2019-05-01T11:10:36.104906: step 315, loss 0.466006, acc 0.875\n",
      "2019-05-01T11:10:36.595083: step 316, loss 0.393914, acc 0.875\n",
      "2019-05-01T11:10:37.176975: step 317, loss 0.475683, acc 0.90625\n",
      "2019-05-01T11:10:37.678413: step 318, loss 0.397891, acc 0.90625\n",
      "2019-05-01T11:10:38.281817: step 319, loss 0.363737, acc 0.875\n",
      "2019-05-01T11:10:38.765210: step 320, loss 0.353058, acc 0.9375\n",
      "2019-05-01T11:10:39.282450: step 321, loss 0.331931, acc 0.96875\n",
      "2019-05-01T11:10:39.801546: step 322, loss 0.482577, acc 0.96875\n",
      "2019-05-01T11:10:40.306801: step 323, loss 0.426565, acc 0.9375\n",
      "2019-05-01T11:10:40.842492: step 324, loss 0.541489, acc 0.9375\n",
      "2019-05-01T11:10:41.313299: step 325, loss 0.380608, acc 1\n",
      "2019-05-01T11:10:41.755436: step 326, loss 0.591618, acc 0.90625\n",
      "2019-05-01T11:10:42.256898: step 327, loss 0.522436, acc 0.875\n",
      "2019-05-01T11:10:42.756492: step 328, loss 0.610294, acc 0.8125\n",
      "2019-05-01T11:10:43.380790: step 329, loss 0.336732, acc 0.96875\n",
      "2019-05-01T11:10:43.839335: step 330, loss 0.418519, acc 0.96875\n",
      "2019-05-01T11:10:44.389128: step 331, loss 0.351003, acc 0.96875\n",
      "2019-05-01T11:10:44.871893: step 332, loss 0.393212, acc 0.875\n",
      "2019-05-01T11:10:45.428894: step 333, loss 0.434153, acc 0.9375\n",
      "2019-05-01T11:10:45.966536: step 334, loss 0.441507, acc 0.96875\n",
      "2019-05-01T11:10:46.449264: step 335, loss 0.572596, acc 0.8125\n",
      "2019-05-01T11:10:46.536283: step 336, loss 0.562325, acc 0.75\n",
      "2019-05-01T11:10:47.110226: step 337, loss 0.278336, acc 1\n",
      "2019-05-01T11:10:47.748692: step 338, loss 0.395957, acc 0.9375\n",
      "2019-05-01T11:10:48.413700: step 339, loss 0.429426, acc 0.90625\n",
      "2019-05-01T11:10:48.887365: step 340, loss 0.300186, acc 0.96875\n",
      "2019-05-01T11:10:49.422621: step 341, loss 0.45089, acc 0.9375\n",
      "2019-05-01T11:10:49.882250: step 342, loss 0.395312, acc 0.9375\n",
      "2019-05-01T11:10:50.346410: step 343, loss 0.41183, acc 0.96875\n",
      "2019-05-01T11:10:50.770989: step 344, loss 0.41233, acc 0.90625\n",
      "2019-05-01T11:10:51.224469: step 345, loss 0.324601, acc 1\n",
      "2019-05-01T11:10:51.695854: step 346, loss 0.30248, acc 1\n",
      "2019-05-01T11:10:52.165569: step 347, loss 0.334299, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:10:52.651427: step 348, loss 0.2873, acc 0.96875\n",
      "2019-05-01T11:10:53.205233: step 349, loss 0.3834, acc 0.9375\n",
      "2019-05-01T11:10:53.769030: step 350, loss 0.30048, acc 1\n",
      "2019-05-01T11:10:54.330056: step 351, loss 0.386971, acc 0.96875\n",
      "2019-05-01T11:10:55.012459: step 352, loss 0.462006, acc 0.9375\n",
      "2019-05-01T11:10:55.511648: step 353, loss 0.542726, acc 0.9375\n",
      "2019-05-01T11:10:56.017655: step 354, loss 0.389169, acc 0.96875\n",
      "2019-05-01T11:10:56.625317: step 355, loss 0.324909, acc 1\n",
      "2019-05-01T11:10:57.070262: step 356, loss 0.286211, acc 1\n",
      "2019-05-01T11:10:57.532897: step 357, loss 0.384909, acc 0.9375\n",
      "2019-05-01T11:10:57.958639: step 358, loss 0.436537, acc 0.875\n",
      "2019-05-01T11:10:58.411206: step 359, loss 0.369691, acc 0.875\n",
      "2019-05-01T11:10:58.487760: step 360, loss 0.388802, acc 1\n",
      "2019-05-01T11:10:58.909042: step 361, loss 0.324083, acc 0.96875\n",
      "2019-05-01T11:10:59.353863: step 362, loss 0.245815, acc 1\n",
      "2019-05-01T11:10:59.794370: step 363, loss 0.324808, acc 0.96875\n",
      "2019-05-01T11:11:00.264397: step 364, loss 0.396649, acc 0.96875\n",
      "2019-05-01T11:11:00.689264: step 365, loss 0.3207, acc 0.96875\n",
      "2019-05-01T11:11:01.123743: step 366, loss 0.357936, acc 0.90625\n",
      "2019-05-01T11:11:01.561176: step 367, loss 0.296058, acc 1\n",
      "2019-05-01T11:11:01.991666: step 368, loss 0.318081, acc 0.96875\n",
      "2019-05-01T11:11:02.437696: step 369, loss 0.275511, acc 0.96875\n",
      "2019-05-01T11:11:02.898320: step 370, loss 0.283849, acc 0.9375\n",
      "2019-05-01T11:11:03.346835: step 371, loss 0.360637, acc 0.96875\n",
      "2019-05-01T11:11:03.779510: step 372, loss 0.325489, acc 0.96875\n",
      "2019-05-01T11:11:04.250330: step 373, loss 0.306495, acc 1\n",
      "2019-05-01T11:11:04.722868: step 374, loss 0.319586, acc 0.96875\n",
      "2019-05-01T11:11:05.176717: step 375, loss 0.315846, acc 0.96875\n",
      "2019-05-01T11:11:05.633805: step 376, loss 0.257018, acc 1\n",
      "2019-05-01T11:11:06.104044: step 377, loss 0.37727, acc 0.9375\n",
      "2019-05-01T11:11:06.579340: step 378, loss 0.288091, acc 0.96875\n",
      "2019-05-01T11:11:07.003449: step 379, loss 0.370043, acc 0.96875\n",
      "2019-05-01T11:11:07.442380: step 380, loss 0.385052, acc 0.9375\n",
      "2019-05-01T11:11:07.930272: step 381, loss 0.315008, acc 1\n",
      "2019-05-01T11:11:08.458846: step 382, loss 0.393033, acc 0.90625\n",
      "2019-05-01T11:11:08.928881: step 383, loss 0.318946, acc 0.96875\n",
      "2019-05-01T11:11:09.010897: step 384, loss 0.296514, acc 1\n",
      "2019-05-01T11:11:09.445999: step 385, loss 0.33896, acc 0.96875\n",
      "2019-05-01T11:11:09.866033: step 386, loss 0.247484, acc 1\n",
      "2019-05-01T11:11:10.301279: step 387, loss 0.330644, acc 0.96875\n",
      "2019-05-01T11:11:10.731986: step 388, loss 0.250988, acc 0.96875\n",
      "2019-05-01T11:11:11.180227: step 389, loss 0.285684, acc 0.96875\n",
      "2019-05-01T11:11:11.632929: step 390, loss 0.29871, acc 1\n",
      "2019-05-01T11:11:12.109077: step 391, loss 0.336936, acc 0.96875\n",
      "2019-05-01T11:11:12.587625: step 392, loss 0.272221, acc 1\n",
      "2019-05-01T11:11:13.131546: step 393, loss 0.30829, acc 1\n",
      "2019-05-01T11:11:13.612654: step 394, loss 0.232977, acc 1\n",
      "2019-05-01T11:11:14.073172: step 395, loss 0.213921, acc 1\n",
      "2019-05-01T11:11:14.532949: step 396, loss 0.433678, acc 0.90625\n",
      "2019-05-01T11:11:14.955964: step 397, loss 0.223738, acc 0.96875\n",
      "2019-05-01T11:11:15.425977: step 398, loss 0.251647, acc 1\n",
      "2019-05-01T11:11:15.847741: step 399, loss 0.274346, acc 1\n",
      "2019-05-01T11:11:16.327405: step 400, loss 0.269416, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:11:17.194044: step 400, loss 2.33385, acc 0.124324\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-400\n",
      "\n",
      "2019-05-01T11:11:17.692134: step 401, loss 0.251942, acc 0.96875\n",
      "2019-05-01T11:11:18.138990: step 402, loss 0.343429, acc 0.9375\n",
      "2019-05-01T11:11:18.570068: step 403, loss 0.282814, acc 0.96875\n",
      "2019-05-01T11:11:18.989293: step 404, loss 0.312469, acc 0.96875\n",
      "2019-05-01T11:11:19.448648: step 405, loss 0.182781, acc 1\n",
      "2019-05-01T11:11:19.866034: step 406, loss 0.34901, acc 0.90625\n",
      "2019-05-01T11:11:20.325717: step 407, loss 0.175668, acc 1\n",
      "2019-05-01T11:11:20.394640: step 408, loss 0.275517, acc 1\n",
      "2019-05-01T11:11:20.819321: step 409, loss 0.371636, acc 0.96875\n",
      "2019-05-01T11:11:21.327936: step 410, loss 0.399918, acc 0.90625\n",
      "2019-05-01T11:11:21.917221: step 411, loss 0.362857, acc 0.9375\n",
      "2019-05-01T11:11:22.608026: step 412, loss 0.224311, acc 0.96875\n",
      "2019-05-01T11:11:23.191396: step 413, loss 0.218213, acc 0.96875\n",
      "2019-05-01T11:11:23.687541: step 414, loss 0.240037, acc 1\n",
      "2019-05-01T11:11:24.124095: step 415, loss 0.284014, acc 0.96875\n",
      "2019-05-01T11:11:24.570036: step 416, loss 0.338189, acc 0.9375\n",
      "2019-05-01T11:11:25.020181: step 417, loss 0.208501, acc 1\n",
      "2019-05-01T11:11:25.508882: step 418, loss 0.271126, acc 0.96875\n",
      "2019-05-01T11:11:25.950296: step 419, loss 0.339007, acc 0.9375\n",
      "2019-05-01T11:11:26.455970: step 420, loss 0.329222, acc 0.96875\n",
      "2019-05-01T11:11:26.918491: step 421, loss 0.262695, acc 1\n",
      "2019-05-01T11:11:27.368811: step 422, loss 0.279914, acc 0.96875\n",
      "2019-05-01T11:11:27.790508: step 423, loss 0.242764, acc 1\n",
      "2019-05-01T11:11:28.267760: step 424, loss 0.319989, acc 0.96875\n",
      "2019-05-01T11:11:28.696117: step 425, loss 0.254883, acc 0.96875\n",
      "2019-05-01T11:11:29.146917: step 426, loss 0.292726, acc 0.9375\n",
      "2019-05-01T11:11:29.617950: step 427, loss 0.288574, acc 0.9375\n",
      "2019-05-01T11:11:30.043877: step 428, loss 0.244524, acc 0.9375\n",
      "2019-05-01T11:11:30.535748: step 429, loss 0.237131, acc 1\n",
      "2019-05-01T11:11:30.951196: step 430, loss 0.238142, acc 0.96875\n",
      "2019-05-01T11:11:31.433536: step 431, loss 0.274167, acc 0.96875\n",
      "2019-05-01T11:11:31.506200: step 432, loss 0.215436, acc 1\n",
      "2019-05-01T11:11:31.941580: step 433, loss 0.235958, acc 1\n",
      "2019-05-01T11:11:32.393201: step 434, loss 0.289383, acc 1\n",
      "2019-05-01T11:11:32.824286: step 435, loss 0.24162, acc 0.96875\n",
      "2019-05-01T11:11:33.263259: step 436, loss 0.309103, acc 0.96875\n",
      "2019-05-01T11:11:33.718384: step 437, loss 0.258682, acc 1\n",
      "2019-05-01T11:11:34.154290: step 438, loss 0.135895, acc 1\n",
      "2019-05-01T11:11:34.595059: step 439, loss 0.230018, acc 0.96875\n",
      "2019-05-01T11:11:35.024144: step 440, loss 0.288352, acc 1\n",
      "2019-05-01T11:11:35.481528: step 441, loss 0.268179, acc 0.96875\n",
      "2019-05-01T11:11:35.901869: step 442, loss 0.235036, acc 0.96875\n",
      "2019-05-01T11:11:36.345366: step 443, loss 0.273846, acc 0.96875\n",
      "2019-05-01T11:11:36.810365: step 444, loss 0.242551, acc 1\n",
      "2019-05-01T11:11:37.243729: step 445, loss 0.217002, acc 1\n",
      "2019-05-01T11:11:37.673757: step 446, loss 0.202818, acc 1\n",
      "2019-05-01T11:11:38.144454: step 447, loss 0.277113, acc 0.9375\n",
      "2019-05-01T11:11:38.580057: step 448, loss 0.276412, acc 0.96875\n",
      "2019-05-01T11:11:38.997755: step 449, loss 0.280005, acc 1\n",
      "2019-05-01T11:11:39.438650: step 450, loss 0.275966, acc 0.96875\n",
      "2019-05-01T11:11:39.861364: step 451, loss 0.221974, acc 0.9375\n",
      "2019-05-01T11:11:40.307982: step 452, loss 0.226159, acc 1\n",
      "2019-05-01T11:11:40.733530: step 453, loss 0.228401, acc 0.96875\n",
      "2019-05-01T11:11:41.170196: step 454, loss 0.24676, acc 1\n",
      "2019-05-01T11:11:41.611139: step 455, loss 0.325395, acc 0.96875\n",
      "2019-05-01T11:11:41.690349: step 456, loss 0.0222442, acc 1\n",
      "2019-05-01T11:11:42.192650: step 457, loss 0.235497, acc 0.96875\n",
      "2019-05-01T11:11:42.635665: step 458, loss 0.183758, acc 0.96875\n",
      "2019-05-01T11:11:43.084050: step 459, loss 0.183437, acc 1\n",
      "2019-05-01T11:11:43.541934: step 460, loss 0.156333, acc 1\n",
      "2019-05-01T11:11:43.966855: step 461, loss 0.1664, acc 1\n",
      "2019-05-01T11:11:44.462447: step 462, loss 0.213359, acc 1\n",
      "2019-05-01T11:11:44.934488: step 463, loss 0.267551, acc 0.96875\n",
      "2019-05-01T11:11:45.733989: step 464, loss 0.308247, acc 0.96875\n",
      "2019-05-01T11:11:46.407474: step 465, loss 0.282272, acc 0.9375\n",
      "2019-05-01T11:11:46.977062: step 466, loss 0.161052, acc 1\n",
      "2019-05-01T11:11:47.458366: step 467, loss 0.294186, acc 0.96875\n",
      "2019-05-01T11:11:47.932802: step 468, loss 0.191805, acc 0.96875\n",
      "2019-05-01T11:11:48.374498: step 469, loss 0.177565, acc 0.96875\n",
      "2019-05-01T11:11:48.796037: step 470, loss 0.164254, acc 1\n",
      "2019-05-01T11:11:49.240986: step 471, loss 0.241464, acc 0.96875\n",
      "2019-05-01T11:11:49.670628: step 472, loss 0.229563, acc 1\n",
      "2019-05-01T11:11:50.139759: step 473, loss 0.190021, acc 1\n",
      "2019-05-01T11:11:50.604762: step 474, loss 0.230841, acc 0.96875\n",
      "2019-05-01T11:11:51.049398: step 475, loss 0.383454, acc 0.9375\n",
      "2019-05-01T11:11:51.485691: step 476, loss 0.143179, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:11:51.906994: step 477, loss 0.275838, acc 0.9375\n",
      "2019-05-01T11:11:52.454242: step 478, loss 0.287466, acc 0.96875\n",
      "2019-05-01T11:11:53.042863: step 479, loss 0.320016, acc 0.96875\n",
      "2019-05-01T11:11:53.143382: step 480, loss 0.0981565, acc 1\n",
      "2019-05-01T11:11:53.991691: step 481, loss 0.174418, acc 1\n",
      "2019-05-01T11:11:54.676910: step 482, loss 0.175928, acc 1\n",
      "2019-05-01T11:11:56.031925: step 483, loss 0.193638, acc 1\n",
      "2019-05-01T11:11:57.081430: step 484, loss 0.183192, acc 1\n",
      "2019-05-01T11:11:57.747136: step 485, loss 0.171804, acc 0.96875\n",
      "2019-05-01T11:11:58.262200: step 486, loss 0.165078, acc 1\n",
      "2019-05-01T11:11:58.763999: step 487, loss 0.115928, acc 1\n",
      "2019-05-01T11:11:59.261441: step 488, loss 0.149395, acc 1\n",
      "2019-05-01T11:11:59.917881: step 489, loss 0.28615, acc 0.9375\n",
      "2019-05-01T11:12:00.365622: step 490, loss 0.193485, acc 1\n",
      "2019-05-01T11:12:00.793330: step 491, loss 0.280363, acc 1\n",
      "2019-05-01T11:12:01.242519: step 492, loss 0.204141, acc 1\n",
      "2019-05-01T11:12:01.678780: step 493, loss 0.290735, acc 0.96875\n",
      "2019-05-01T11:12:02.129277: step 494, loss 0.326417, acc 1\n",
      "2019-05-01T11:12:02.586975: step 495, loss 0.279334, acc 0.9375\n",
      "2019-05-01T11:12:03.045467: step 496, loss 0.16288, acc 1\n",
      "2019-05-01T11:12:03.483756: step 497, loss 0.154202, acc 1\n",
      "2019-05-01T11:12:03.974831: step 498, loss 0.221257, acc 1\n",
      "2019-05-01T11:12:04.434121: step 499, loss 0.209232, acc 0.96875\n",
      "2019-05-01T11:12:04.877015: step 500, loss 0.187723, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:12:05.651602: step 500, loss 2.3425, acc 0.102703\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-500\n",
      "\n",
      "2019-05-01T11:12:06.964470: step 501, loss 0.117096, acc 1\n",
      "2019-05-01T11:12:08.039405: step 502, loss 0.301715, acc 0.96875\n",
      "2019-05-01T11:12:09.018176: step 503, loss 0.199421, acc 0.96875\n",
      "2019-05-01T11:12:09.174999: step 504, loss 0.0967915, acc 1\n",
      "2019-05-01T11:12:10.217544: step 505, loss 0.189715, acc 1\n",
      "2019-05-01T11:12:11.252193: step 506, loss 0.148414, acc 1\n",
      "2019-05-01T11:12:12.082268: step 507, loss 0.137005, acc 1\n",
      "2019-05-01T11:12:13.381110: step 508, loss 0.21776, acc 1\n",
      "2019-05-01T11:12:14.454426: step 509, loss 0.242677, acc 0.96875\n",
      "2019-05-01T11:12:15.494549: step 510, loss 0.320091, acc 0.9375\n",
      "2019-05-01T11:12:16.437196: step 511, loss 0.162539, acc 0.96875\n",
      "2019-05-01T11:12:17.365483: step 512, loss 0.195583, acc 1\n",
      "2019-05-01T11:12:18.508259: step 513, loss 0.120994, acc 1\n",
      "2019-05-01T11:12:19.808855: step 514, loss 0.178358, acc 0.96875\n",
      "2019-05-01T11:12:20.810606: step 515, loss 0.119431, acc 1\n",
      "2019-05-01T11:12:21.905152: step 516, loss 0.152703, acc 1\n",
      "2019-05-01T11:12:22.951815: step 517, loss 0.269872, acc 0.9375\n",
      "2019-05-01T11:12:24.181466: step 518, loss 0.215564, acc 1\n",
      "2019-05-01T11:12:25.385571: step 519, loss 0.272435, acc 0.9375\n",
      "2019-05-01T11:12:26.651161: step 520, loss 0.197814, acc 1\n",
      "2019-05-01T11:12:27.692157: step 521, loss 0.264341, acc 0.9375\n",
      "2019-05-01T11:12:29.128997: step 522, loss 0.203013, acc 1\n",
      "2019-05-01T11:12:30.356409: step 523, loss 0.194729, acc 1\n",
      "2019-05-01T11:12:31.472113: step 524, loss 0.195384, acc 0.96875\n",
      "2019-05-01T11:12:32.402451: step 525, loss 0.184529, acc 1\n",
      "2019-05-01T11:12:33.422549: step 526, loss 0.185692, acc 0.96875\n",
      "2019-05-01T11:12:34.325834: step 527, loss 0.175695, acc 0.96875\n",
      "2019-05-01T11:12:34.484267: step 528, loss 0.0134034, acc 1\n",
      "2019-05-01T11:12:35.562732: step 529, loss 0.177654, acc 1\n",
      "2019-05-01T11:12:36.670975: step 530, loss 0.248517, acc 0.9375\n",
      "2019-05-01T11:12:37.613997: step 531, loss 0.128795, acc 1\n",
      "2019-05-01T11:12:38.803356: step 532, loss 0.158818, acc 1\n",
      "2019-05-01T11:12:39.813335: step 533, loss 0.182754, acc 1\n",
      "2019-05-01T11:12:41.034053: step 534, loss 0.245515, acc 0.96875\n",
      "2019-05-01T11:12:42.375704: step 535, loss 0.142866, acc 1\n",
      "2019-05-01T11:12:43.248701: step 536, loss 0.287254, acc 0.96875\n",
      "2019-05-01T11:12:44.430390: step 537, loss 0.143223, acc 1\n",
      "2019-05-01T11:12:45.579564: step 538, loss 0.0992442, acc 1\n",
      "2019-05-01T11:12:46.739108: step 539, loss 0.234192, acc 0.96875\n",
      "2019-05-01T11:12:48.122071: step 540, loss 0.134798, acc 1\n",
      "2019-05-01T11:12:49.324075: step 541, loss 0.137476, acc 1\n",
      "2019-05-01T11:12:50.423932: step 542, loss 0.126251, acc 1\n",
      "2019-05-01T11:12:51.620879: step 543, loss 0.162171, acc 0.96875\n",
      "2019-05-01T11:12:52.712330: step 544, loss 0.233061, acc 0.90625\n",
      "2019-05-01T11:12:54.018326: step 545, loss 0.127299, acc 1\n",
      "2019-05-01T11:12:55.089076: step 546, loss 0.136844, acc 1\n",
      "2019-05-01T11:12:56.412998: step 547, loss 0.134527, acc 1\n",
      "2019-05-01T11:12:57.810379: step 548, loss 0.150457, acc 0.96875\n",
      "2019-05-01T11:12:58.840488: step 549, loss 0.129759, acc 1\n",
      "2019-05-01T11:13:00.019847: step 550, loss 0.187598, acc 0.96875\n",
      "2019-05-01T11:13:00.989950: step 551, loss 0.143724, acc 1\n",
      "2019-05-01T11:13:01.239352: step 552, loss 0.255886, acc 1\n",
      "2019-05-01T11:13:02.396525: step 553, loss 0.195669, acc 0.96875\n",
      "2019-05-01T11:13:03.409757: step 554, loss 0.161337, acc 0.9375\n",
      "2019-05-01T11:13:04.504226: step 555, loss 0.0970936, acc 1\n",
      "2019-05-01T11:13:05.749742: step 556, loss 0.194756, acc 0.96875\n",
      "2019-05-01T11:13:07.183442: step 557, loss 0.151585, acc 0.96875\n",
      "2019-05-01T11:13:08.184160: step 558, loss 0.139284, acc 1\n",
      "2019-05-01T11:13:09.661282: step 559, loss 0.113578, acc 1\n",
      "2019-05-01T11:13:11.164452: step 560, loss 0.161791, acc 1\n",
      "2019-05-01T11:13:12.309739: step 561, loss 0.181458, acc 0.96875\n",
      "2019-05-01T11:13:13.575223: step 562, loss 0.0956609, acc 1\n",
      "2019-05-01T11:13:14.704346: step 563, loss 0.154913, acc 1\n",
      "2019-05-01T11:13:16.480361: step 564, loss 0.0889555, acc 1\n",
      "2019-05-01T11:13:17.828660: step 565, loss 0.112527, acc 1\n",
      "2019-05-01T11:13:19.336366: step 566, loss 0.137135, acc 1\n",
      "2019-05-01T11:13:20.623482: step 567, loss 0.138999, acc 1\n",
      "2019-05-01T11:13:21.578289: step 568, loss 0.178178, acc 1\n",
      "2019-05-01T11:13:22.570416: step 569, loss 0.153301, acc 0.96875\n",
      "2019-05-01T11:13:24.482776: step 570, loss 0.170235, acc 0.96875\n",
      "2019-05-01T11:13:25.948339: step 571, loss 0.168611, acc 1\n",
      "2019-05-01T11:13:28.612927: step 572, loss 0.15193, acc 1\n",
      "2019-05-01T11:13:30.640147: step 573, loss 0.224006, acc 0.96875\n",
      "2019-05-01T11:13:31.915475: step 574, loss 0.181531, acc 0.96875\n",
      "2019-05-01T11:13:33.087125: step 575, loss 0.181426, acc 1\n",
      "2019-05-01T11:13:33.414697: step 576, loss 0.0514158, acc 1\n",
      "2019-05-01T11:13:34.777940: step 577, loss 0.113161, acc 1\n",
      "2019-05-01T11:13:35.956167: step 578, loss 0.186435, acc 1\n",
      "2019-05-01T11:13:37.097391: step 579, loss 0.188441, acc 0.90625\n",
      "2019-05-01T11:13:38.438455: step 580, loss 0.103798, acc 1\n",
      "2019-05-01T11:13:39.519919: step 581, loss 0.244842, acc 1\n",
      "2019-05-01T11:13:40.553806: step 582, loss 0.109539, acc 1\n",
      "2019-05-01T11:13:41.462326: step 583, loss 0.135753, acc 1\n",
      "2019-05-01T11:13:42.621649: step 584, loss 0.108711, acc 1\n",
      "2019-05-01T11:13:43.883871: step 585, loss 0.276522, acc 0.96875\n",
      "2019-05-01T11:13:45.341112: step 586, loss 0.157546, acc 1\n",
      "2019-05-01T11:13:46.763214: step 587, loss 0.102971, acc 1\n",
      "2019-05-01T11:13:48.481372: step 588, loss 0.136033, acc 1\n",
      "2019-05-01T11:13:49.834261: step 589, loss 0.193615, acc 1\n",
      "2019-05-01T11:13:50.795902: step 590, loss 0.0957709, acc 1\n",
      "2019-05-01T11:13:52.105270: step 591, loss 0.138615, acc 1\n",
      "2019-05-01T11:13:53.177651: step 592, loss 0.127847, acc 1\n",
      "2019-05-01T11:13:55.024631: step 593, loss 0.169848, acc 0.96875\n",
      "2019-05-01T11:13:56.426844: step 594, loss 0.153296, acc 1\n",
      "2019-05-01T11:13:57.582097: step 595, loss 0.202963, acc 0.96875\n",
      "2019-05-01T11:13:58.797298: step 596, loss 0.221808, acc 0.96875\n",
      "2019-05-01T11:14:00.004569: step 597, loss 0.143971, acc 1\n",
      "2019-05-01T11:14:01.094612: step 598, loss 0.12874, acc 1\n",
      "2019-05-01T11:14:02.283583: step 599, loss 0.147599, acc 1\n",
      "2019-05-01T11:14:02.500138: step 600, loss 0.694627, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:14:04.253686: step 600, loss 2.34442, acc 0.145946\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-600\n",
      "\n",
      "2019-05-01T11:14:05.265457: step 601, loss 0.193846, acc 0.96875\n",
      "2019-05-01T11:14:06.578109: step 602, loss 0.122586, acc 1\n",
      "2019-05-01T11:14:07.685992: step 603, loss 0.136079, acc 0.96875\n",
      "2019-05-01T11:14:08.911689: step 604, loss 0.198281, acc 0.96875\n",
      "2019-05-01T11:14:09.987758: step 605, loss 0.186653, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:14:11.163453: step 606, loss 0.165813, acc 1\n",
      "2019-05-01T11:14:12.433694: step 607, loss 0.282069, acc 0.9375\n",
      "2019-05-01T11:14:13.614747: step 608, loss 0.121859, acc 1\n",
      "2019-05-01T11:14:14.641025: step 609, loss 0.225539, acc 0.9375\n",
      "2019-05-01T11:14:15.789944: step 610, loss 0.0886985, acc 1\n",
      "2019-05-01T11:14:16.970530: step 611, loss 0.121258, acc 1\n",
      "2019-05-01T11:14:18.323539: step 612, loss 0.245603, acc 0.96875\n",
      "2019-05-01T11:14:19.360570: step 613, loss 0.179954, acc 0.96875\n",
      "2019-05-01T11:14:20.492570: step 614, loss 0.168205, acc 1\n",
      "2019-05-01T11:14:21.459973: step 615, loss 0.164258, acc 0.96875\n",
      "2019-05-01T11:14:22.426496: step 616, loss 0.0939523, acc 1\n",
      "2019-05-01T11:14:23.505792: step 617, loss 0.133638, acc 1\n",
      "2019-05-01T11:14:24.592297: step 618, loss 0.16565, acc 0.96875\n",
      "2019-05-01T11:14:25.732682: step 619, loss 0.189364, acc 1\n",
      "2019-05-01T11:14:27.019557: step 620, loss 0.152033, acc 1\n",
      "2019-05-01T11:14:28.014269: step 621, loss 0.134845, acc 1\n",
      "2019-05-01T11:14:29.170121: step 622, loss 0.136531, acc 1\n",
      "2019-05-01T11:14:30.555383: step 623, loss 0.213057, acc 1\n",
      "2019-05-01T11:14:30.769553: step 624, loss 0.0462531, acc 1\n",
      "2019-05-01T11:14:31.901201: step 625, loss 0.184674, acc 0.96875\n",
      "2019-05-01T11:14:33.448693: step 626, loss 0.164059, acc 1\n",
      "2019-05-01T11:14:34.792136: step 627, loss 0.0827352, acc 1\n",
      "2019-05-01T11:14:36.088011: step 628, loss 0.144167, acc 1\n",
      "2019-05-01T11:14:37.426577: step 629, loss 0.124694, acc 1\n",
      "2019-05-01T11:14:38.739990: step 630, loss 0.173, acc 0.96875\n",
      "2019-05-01T11:14:40.161094: step 631, loss 0.191845, acc 0.96875\n",
      "2019-05-01T11:14:41.505784: step 632, loss 0.209282, acc 1\n",
      "2019-05-01T11:14:42.581019: step 633, loss 0.0968729, acc 1\n",
      "2019-05-01T11:14:44.149353: step 634, loss 0.139365, acc 1\n",
      "2019-05-01T11:14:45.715813: step 635, loss 0.0849623, acc 1\n",
      "2019-05-01T11:14:46.877737: step 636, loss 0.115013, acc 1\n",
      "2019-05-01T11:14:48.157227: step 637, loss 0.0742719, acc 1\n",
      "2019-05-01T11:14:49.092532: step 638, loss 0.124119, acc 1\n",
      "2019-05-01T11:14:50.355411: step 639, loss 0.29824, acc 0.9375\n",
      "2019-05-01T11:14:51.288558: step 640, loss 0.106178, acc 1\n",
      "2019-05-01T11:14:52.212087: step 641, loss 0.0956919, acc 1\n",
      "2019-05-01T11:14:53.116033: step 642, loss 0.132891, acc 1\n",
      "2019-05-01T11:14:54.155338: step 643, loss 0.122844, acc 1\n",
      "2019-05-01T11:14:55.172424: step 644, loss 0.128821, acc 1\n",
      "2019-05-01T11:14:56.334126: step 645, loss 0.0975634, acc 1\n",
      "2019-05-01T11:14:57.467031: step 646, loss 0.162476, acc 1\n",
      "2019-05-01T11:14:58.717668: step 647, loss 0.123897, acc 1\n",
      "2019-05-01T11:14:58.907611: step 648, loss 0.0921917, acc 1\n",
      "2019-05-01T11:14:59.848106: step 649, loss 0.115205, acc 1\n",
      "2019-05-01T11:15:00.884632: step 650, loss 0.163771, acc 0.96875\n",
      "2019-05-01T11:15:02.068886: step 651, loss 0.111584, acc 0.96875\n",
      "2019-05-01T11:15:03.074633: step 652, loss 0.155907, acc 0.96875\n",
      "2019-05-01T11:15:04.030008: step 653, loss 0.108723, acc 1\n",
      "2019-05-01T11:15:05.190455: step 654, loss 0.212948, acc 0.96875\n",
      "2019-05-01T11:15:06.223128: step 655, loss 0.114775, acc 0.96875\n",
      "2019-05-01T11:15:07.331191: step 656, loss 0.151073, acc 1\n",
      "2019-05-01T11:15:08.490884: step 657, loss 0.146488, acc 1\n",
      "2019-05-01T11:15:09.647028: step 658, loss 0.13204, acc 1\n",
      "2019-05-01T11:15:10.924726: step 659, loss 0.0892611, acc 1\n",
      "2019-05-01T11:15:12.070297: step 660, loss 0.143957, acc 1\n",
      "2019-05-01T11:15:13.196364: step 661, loss 0.111156, acc 1\n",
      "2019-05-01T11:15:14.331830: step 662, loss 0.17029, acc 1\n",
      "2019-05-01T11:15:15.191910: step 663, loss 0.0980743, acc 1\n",
      "2019-05-01T11:15:16.320072: step 664, loss 0.193971, acc 0.96875\n",
      "2019-05-01T11:15:17.461693: step 665, loss 0.127271, acc 0.96875\n",
      "2019-05-01T11:15:18.638589: step 666, loss 0.100066, acc 1\n",
      "2019-05-01T11:15:19.636056: step 667, loss 0.111449, acc 0.96875\n",
      "2019-05-01T11:15:20.875112: step 668, loss 0.0772612, acc 1\n",
      "2019-05-01T11:15:22.000422: step 669, loss 0.0903773, acc 1\n",
      "2019-05-01T11:15:23.459485: step 670, loss 0.201047, acc 0.9375\n",
      "2019-05-01T11:15:24.603532: step 671, loss 0.113119, acc 1\n",
      "2019-05-01T11:15:24.861228: step 672, loss 0.806245, acc 0.75\n",
      "2019-05-01T11:15:26.000219: step 673, loss 0.205478, acc 0.96875\n",
      "2019-05-01T11:15:27.112718: step 674, loss 0.0803905, acc 1\n",
      "2019-05-01T11:15:28.503063: step 675, loss 0.0967118, acc 1\n",
      "2019-05-01T11:15:29.397292: step 676, loss 0.180406, acc 0.96875\n",
      "2019-05-01T11:15:30.438050: step 677, loss 0.115123, acc 1\n",
      "2019-05-01T11:15:31.831199: step 678, loss 0.139217, acc 1\n",
      "2019-05-01T11:15:32.900655: step 679, loss 0.17628, acc 1\n",
      "2019-05-01T11:15:33.941481: step 680, loss 0.116579, acc 1\n",
      "2019-05-01T11:15:35.136219: step 681, loss 0.109179, acc 1\n",
      "2019-05-01T11:15:36.345584: step 682, loss 0.195777, acc 0.96875\n",
      "2019-05-01T11:15:37.781465: step 683, loss 0.203859, acc 0.96875\n",
      "2019-05-01T11:15:39.057307: step 684, loss 0.0760727, acc 1\n",
      "2019-05-01T11:15:40.321930: step 685, loss 0.140493, acc 0.96875\n",
      "2019-05-01T11:15:41.883880: step 686, loss 0.131657, acc 0.96875\n",
      "2019-05-01T11:15:42.998331: step 687, loss 0.172469, acc 0.96875\n",
      "2019-05-01T11:15:44.637768: step 688, loss 0.0846644, acc 1\n",
      "2019-05-01T11:15:45.899803: step 689, loss 0.0806011, acc 1\n",
      "2019-05-01T11:15:46.934073: step 690, loss 0.149431, acc 1\n",
      "2019-05-01T11:15:49.551381: step 691, loss 0.123673, acc 0.96875\n",
      "2019-05-01T11:15:50.606355: step 692, loss 0.133748, acc 0.96875\n",
      "2019-05-01T11:15:52.246266: step 693, loss 0.10708, acc 1\n",
      "2019-05-01T11:15:53.666960: step 694, loss 0.129962, acc 1\n",
      "2019-05-01T11:15:55.655810: step 695, loss 0.178923, acc 0.96875\n",
      "2019-05-01T11:15:55.938968: step 696, loss 0.473394, acc 1\n",
      "2019-05-01T11:15:57.243597: step 697, loss 0.113315, acc 1\n",
      "2019-05-01T11:15:58.636876: step 698, loss 0.0780727, acc 1\n",
      "2019-05-01T11:15:59.709282: step 699, loss 0.139187, acc 0.96875\n",
      "2019-05-01T11:16:00.683595: step 700, loss 0.110868, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:16:02.517422: step 700, loss 2.45647, acc 0.124324\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-700\n",
      "\n",
      "2019-05-01T11:16:03.774330: step 701, loss 0.158993, acc 1\n",
      "2019-05-01T11:16:04.847364: step 702, loss 0.121511, acc 0.96875\n",
      "2019-05-01T11:16:06.032421: step 703, loss 0.189783, acc 0.96875\n",
      "2019-05-01T11:16:07.040282: step 704, loss 0.105007, acc 1\n",
      "2019-05-01T11:16:08.244303: step 705, loss 0.117453, acc 1\n",
      "2019-05-01T11:16:09.843441: step 706, loss 0.144353, acc 1\n",
      "2019-05-01T11:16:10.880747: step 707, loss 0.110086, acc 1\n",
      "2019-05-01T11:16:11.864554: step 708, loss 0.11921, acc 1\n",
      "2019-05-01T11:16:12.799780: step 709, loss 0.151544, acc 0.96875\n",
      "2019-05-01T11:16:13.895277: step 710, loss 0.110716, acc 1\n",
      "2019-05-01T11:16:14.805284: step 711, loss 0.154275, acc 1\n",
      "2019-05-01T11:16:16.001109: step 712, loss 0.160991, acc 0.96875\n",
      "2019-05-01T11:16:17.145300: step 713, loss 0.113739, acc 1\n",
      "2019-05-01T11:16:18.308127: step 714, loss 0.121729, acc 1\n",
      "2019-05-01T11:16:19.700369: step 715, loss 0.101097, acc 1\n",
      "2019-05-01T11:16:20.968039: step 716, loss 0.161091, acc 0.96875\n",
      "2019-05-01T11:16:21.996952: step 717, loss 0.169141, acc 0.96875\n",
      "2019-05-01T11:16:22.907193: step 718, loss 0.102127, acc 1\n",
      "2019-05-01T11:16:24.023311: step 719, loss 0.197886, acc 0.96875\n",
      "2019-05-01T11:16:24.281104: step 720, loss 0.127313, acc 1\n",
      "2019-05-01T11:16:25.536188: step 721, loss 0.0914093, acc 1\n",
      "2019-05-01T11:16:26.533523: step 722, loss 0.122799, acc 1\n",
      "2019-05-01T11:16:27.588583: step 723, loss 0.11738, acc 1\n",
      "2019-05-01T11:16:28.940514: step 724, loss 0.118474, acc 1\n",
      "2019-05-01T11:16:29.987016: step 725, loss 0.140354, acc 1\n",
      "2019-05-01T11:16:31.157609: step 726, loss 0.214486, acc 0.96875\n",
      "2019-05-01T11:16:32.407474: step 727, loss 0.120173, acc 1\n",
      "2019-05-01T11:16:33.343734: step 728, loss 0.106925, acc 1\n",
      "2019-05-01T11:16:34.779834: step 729, loss 0.0726773, acc 1\n",
      "2019-05-01T11:16:36.448221: step 730, loss 0.0901513, acc 1\n",
      "2019-05-01T11:16:37.811083: step 731, loss 0.103705, acc 1\n",
      "2019-05-01T11:16:39.284572: step 732, loss 0.106949, acc 0.96875\n",
      "2019-05-01T11:16:40.712955: step 733, loss 0.123202, acc 1\n",
      "2019-05-01T11:16:41.709765: step 734, loss 0.117269, acc 1\n",
      "2019-05-01T11:16:42.754561: step 735, loss 0.138653, acc 1\n",
      "2019-05-01T11:16:43.864909: step 736, loss 0.109328, acc 1\n",
      "2019-05-01T11:16:45.089036: step 737, loss 0.0812647, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:16:46.222751: step 738, loss 0.173357, acc 0.96875\n",
      "2019-05-01T11:16:47.618052: step 739, loss 0.11316, acc 0.96875\n",
      "2019-05-01T11:16:48.940688: step 740, loss 0.0754216, acc 1\n",
      "2019-05-01T11:16:50.341989: step 741, loss 0.152441, acc 0.96875\n",
      "2019-05-01T11:16:51.423118: step 742, loss 0.182601, acc 0.96875\n",
      "2019-05-01T11:16:52.551078: step 743, loss 0.169577, acc 1\n",
      "2019-05-01T11:16:52.716116: step 744, loss 0.206246, acc 1\n",
      "2019-05-01T11:16:54.366981: step 745, loss 0.0966259, acc 1\n",
      "2019-05-01T11:16:55.507260: step 746, loss 0.0949205, acc 1\n",
      "2019-05-01T11:16:56.462746: step 747, loss 0.0699849, acc 1\n",
      "2019-05-01T11:16:57.407850: step 748, loss 0.0975895, acc 1\n",
      "2019-05-01T11:16:58.620391: step 749, loss 0.0514816, acc 1\n",
      "2019-05-01T11:17:00.055821: step 750, loss 0.108546, acc 1\n",
      "2019-05-01T11:17:01.160832: step 751, loss 0.119475, acc 1\n",
      "2019-05-01T11:17:02.180972: step 752, loss 0.176477, acc 0.9375\n",
      "2019-05-01T11:17:03.412181: step 753, loss 0.0764206, acc 1\n",
      "2019-05-01T11:17:05.081147: step 754, loss 0.0814729, acc 1\n",
      "2019-05-01T11:17:06.412989: step 755, loss 0.0741662, acc 1\n",
      "2019-05-01T11:17:07.800089: step 756, loss 0.13136, acc 1\n",
      "2019-05-01T11:17:08.927116: step 757, loss 0.161336, acc 0.96875\n",
      "2019-05-01T11:17:10.148781: step 758, loss 0.0979123, acc 0.96875\n",
      "2019-05-01T11:17:11.629565: step 759, loss 0.0571277, acc 1\n",
      "2019-05-01T11:17:12.401904: step 760, loss 0.100437, acc 1\n",
      "2019-05-01T11:17:13.807096: step 761, loss 0.10418, acc 1\n",
      "2019-05-01T11:17:15.368921: step 762, loss 0.152768, acc 1\n",
      "2019-05-01T11:17:17.262604: step 763, loss 0.091199, acc 1\n",
      "2019-05-01T11:17:18.489055: step 764, loss 0.138721, acc 1\n",
      "2019-05-01T11:17:20.271568: step 765, loss 0.0832357, acc 1\n",
      "2019-05-01T11:17:21.318049: step 766, loss 0.101883, acc 1\n",
      "2019-05-01T11:17:22.428230: step 767, loss 0.104574, acc 1\n",
      "2019-05-01T11:17:22.751307: step 768, loss 0.0847685, acc 1\n",
      "2019-05-01T11:17:23.946060: step 769, loss 0.0691058, acc 1\n",
      "2019-05-01T11:17:25.358216: step 770, loss 0.0648375, acc 1\n",
      "2019-05-01T11:17:26.592040: step 771, loss 0.097777, acc 1\n",
      "2019-05-01T11:17:27.868047: step 772, loss 0.0952167, acc 0.96875\n",
      "2019-05-01T11:17:28.810686: step 773, loss 0.0895713, acc 1\n",
      "2019-05-01T11:17:29.685778: step 774, loss 0.209097, acc 0.96875\n",
      "2019-05-01T11:17:30.725050: step 775, loss 0.0811432, acc 1\n",
      "2019-05-01T11:17:31.832104: step 776, loss 0.137847, acc 1\n",
      "2019-05-01T11:17:32.752920: step 777, loss 0.0903984, acc 1\n",
      "2019-05-01T11:17:33.713568: step 778, loss 0.0762241, acc 1\n",
      "2019-05-01T11:17:34.644761: step 779, loss 0.135711, acc 1\n",
      "2019-05-01T11:17:35.578898: step 780, loss 0.0821078, acc 1\n",
      "2019-05-01T11:17:36.556287: step 781, loss 0.153071, acc 0.96875\n",
      "2019-05-01T11:17:37.431612: step 782, loss 0.11887, acc 1\n",
      "2019-05-01T11:17:38.342403: step 783, loss 0.0605013, acc 1\n",
      "2019-05-01T11:17:39.235708: step 784, loss 0.0995877, acc 1\n",
      "2019-05-01T11:17:40.178906: step 785, loss 0.0513367, acc 1\n",
      "2019-05-01T11:17:41.256006: step 786, loss 0.100019, acc 1\n",
      "2019-05-01T11:17:42.256174: step 787, loss 0.0851958, acc 1\n",
      "2019-05-01T11:17:43.203825: step 788, loss 0.0799798, acc 1\n",
      "2019-05-01T11:17:44.088598: step 789, loss 0.0932937, acc 1\n",
      "2019-05-01T11:17:45.332423: step 790, loss 0.0790283, acc 1\n",
      "2019-05-01T11:17:46.504941: step 791, loss 0.0945056, acc 1\n",
      "2019-05-01T11:17:46.689641: step 792, loss 0.153319, acc 1\n",
      "2019-05-01T11:17:47.898393: step 793, loss 0.0876081, acc 1\n",
      "2019-05-01T11:17:49.062224: step 794, loss 0.0659603, acc 1\n",
      "2019-05-01T11:17:50.364664: step 795, loss 0.0646662, acc 1\n",
      "2019-05-01T11:17:51.556286: step 796, loss 0.0496492, acc 1\n",
      "2019-05-01T11:17:52.608397: step 797, loss 0.108532, acc 1\n",
      "2019-05-01T11:17:53.634580: step 798, loss 0.0765231, acc 1\n",
      "2019-05-01T11:17:54.551097: step 799, loss 0.142006, acc 0.96875\n",
      "2019-05-01T11:17:55.682175: step 800, loss 0.111684, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:17:57.562860: step 800, loss 2.43376, acc 0.140541\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-800\n",
      "\n",
      "2019-05-01T11:17:58.938883: step 801, loss 0.0572714, acc 1\n",
      "2019-05-01T11:18:00.221223: step 802, loss 0.0887077, acc 1\n",
      "2019-05-01T11:18:01.410982: step 803, loss 0.168549, acc 0.96875\n",
      "2019-05-01T11:18:02.764842: step 804, loss 0.250306, acc 0.96875\n",
      "2019-05-01T11:18:04.021032: step 805, loss 0.0827219, acc 1\n",
      "2019-05-01T11:18:05.100098: step 806, loss 0.0476624, acc 1\n",
      "2019-05-01T11:18:06.180913: step 807, loss 0.0677199, acc 1\n",
      "2019-05-01T11:18:07.550302: step 808, loss 0.0882702, acc 1\n",
      "2019-05-01T11:18:08.671116: step 809, loss 0.0998007, acc 1\n",
      "2019-05-01T11:18:09.598595: step 810, loss 0.0499794, acc 1\n",
      "2019-05-01T11:18:10.645767: step 811, loss 0.102888, acc 1\n",
      "2019-05-01T11:18:11.706480: step 812, loss 0.0388822, acc 1\n",
      "2019-05-01T11:18:12.774883: step 813, loss 0.100576, acc 1\n",
      "2019-05-01T11:18:13.896635: step 814, loss 0.0648975, acc 1\n",
      "2019-05-01T11:18:15.062083: step 815, loss 0.146256, acc 1\n",
      "2019-05-01T11:18:15.264008: step 816, loss 0.154198, acc 1\n",
      "2019-05-01T11:18:16.250885: step 817, loss 0.0754821, acc 1\n",
      "2019-05-01T11:18:17.366660: step 818, loss 0.0872949, acc 1\n",
      "2019-05-01T11:18:18.364450: step 819, loss 0.0675636, acc 1\n",
      "2019-05-01T11:18:19.523069: step 820, loss 0.0493786, acc 1\n",
      "2019-05-01T11:18:20.486068: step 821, loss 0.0969557, acc 1\n",
      "2019-05-01T11:18:21.539750: step 822, loss 0.0539407, acc 1\n",
      "2019-05-01T11:18:22.485033: step 823, loss 0.0727959, acc 1\n",
      "2019-05-01T11:18:23.544568: step 824, loss 0.119707, acc 1\n",
      "2019-05-01T11:18:24.502655: step 825, loss 0.179533, acc 0.9375\n",
      "2019-05-01T11:18:25.778970: step 826, loss 0.0927705, acc 1\n",
      "2019-05-01T11:18:27.023516: step 827, loss 0.0919804, acc 1\n",
      "2019-05-01T11:18:28.056221: step 828, loss 0.127724, acc 0.96875\n",
      "2019-05-01T11:18:29.099263: step 829, loss 0.138807, acc 0.96875\n",
      "2019-05-01T11:18:30.431905: step 830, loss 0.0748273, acc 1\n",
      "2019-05-01T11:18:31.495427: step 831, loss 0.0801339, acc 1\n",
      "2019-05-01T11:18:32.650326: step 832, loss 0.0838336, acc 1\n",
      "2019-05-01T11:18:33.618446: step 833, loss 0.253096, acc 0.96875\n",
      "2019-05-01T11:18:34.727361: step 834, loss 0.116382, acc 1\n",
      "2019-05-01T11:18:35.846356: step 835, loss 0.0807461, acc 1\n",
      "2019-05-01T11:18:37.261409: step 836, loss 0.101204, acc 1\n",
      "2019-05-01T11:18:38.190726: step 837, loss 0.0544933, acc 1\n",
      "2019-05-01T11:18:39.545022: step 838, loss 0.0700865, acc 1\n",
      "2019-05-01T11:18:40.664874: step 839, loss 0.0981216, acc 1\n",
      "2019-05-01T11:18:40.854824: step 840, loss 0.137355, acc 1\n",
      "2019-05-01T11:18:41.870007: step 841, loss 0.0332458, acc 1\n",
      "2019-05-01T11:18:42.927431: step 842, loss 0.076115, acc 1\n",
      "2019-05-01T11:18:43.985667: step 843, loss 0.106776, acc 0.96875\n",
      "2019-05-01T11:18:44.852993: step 844, loss 0.0705027, acc 1\n",
      "2019-05-01T11:18:46.103175: step 845, loss 0.1032, acc 1\n",
      "2019-05-01T11:18:47.302332: step 846, loss 0.0649587, acc 1\n",
      "2019-05-01T11:18:48.951192: step 847, loss 0.0476556, acc 1\n",
      "2019-05-01T11:18:50.622655: step 848, loss 0.0795331, acc 1\n",
      "2019-05-01T11:18:52.128061: step 849, loss 0.0778474, acc 1\n",
      "2019-05-01T11:18:53.633532: step 850, loss 0.0833393, acc 1\n",
      "2019-05-01T11:18:54.873519: step 851, loss 0.0681132, acc 1\n",
      "2019-05-01T11:18:56.318129: step 852, loss 0.0808241, acc 1\n",
      "2019-05-01T11:18:57.287389: step 853, loss 0.0983899, acc 0.96875\n",
      "2019-05-01T11:18:57.853271: step 854, loss 0.0890093, acc 1\n",
      "2019-05-01T11:18:58.412712: step 855, loss 0.0452965, acc 1\n",
      "2019-05-01T11:18:58.950954: step 856, loss 0.0738902, acc 1\n",
      "2019-05-01T11:18:59.483716: step 857, loss 0.101137, acc 1\n",
      "2019-05-01T11:19:00.120944: step 858, loss 0.214144, acc 0.96875\n",
      "2019-05-01T11:19:00.632647: step 859, loss 0.127933, acc 1\n",
      "2019-05-01T11:19:01.153995: step 860, loss 0.0588608, acc 1\n",
      "2019-05-01T11:19:01.791718: step 861, loss 0.0423392, acc 1\n",
      "2019-05-01T11:19:02.335063: step 862, loss 0.061461, acc 1\n",
      "2019-05-01T11:19:02.881037: step 863, loss 0.0619851, acc 1\n",
      "2019-05-01T11:19:02.972007: step 864, loss 0.143544, acc 1\n",
      "2019-05-01T11:19:03.599857: step 865, loss 0.0864693, acc 1\n",
      "2019-05-01T11:19:04.109289: step 866, loss 0.0931647, acc 1\n",
      "2019-05-01T11:19:04.664274: step 867, loss 0.0580496, acc 1\n",
      "2019-05-01T11:19:05.226020: step 868, loss 0.0933836, acc 0.96875\n",
      "2019-05-01T11:19:05.830426: step 869, loss 0.0476923, acc 1\n",
      "2019-05-01T11:19:06.409940: step 870, loss 0.103494, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-01T11:19:06.940892: step 871, loss 0.0301676, acc 1\n",
      "2019-05-01T11:19:07.513111: step 872, loss 0.051703, acc 1\n",
      "2019-05-01T11:19:08.085897: step 873, loss 0.074955, acc 1\n",
      "2019-05-01T11:19:09.481068: step 874, loss 0.0928318, acc 1\n",
      "2019-05-01T11:19:10.795303: step 875, loss 0.0410905, acc 1\n",
      "2019-05-01T11:19:12.140303: step 876, loss 0.0936733, acc 1\n",
      "2019-05-01T11:19:12.671451: step 877, loss 0.0776053, acc 1\n",
      "2019-05-01T11:19:14.165369: step 878, loss 0.0581822, acc 1\n",
      "2019-05-01T11:19:15.845844: step 879, loss 0.0588061, acc 1\n",
      "2019-05-01T11:19:16.853839: step 880, loss 0.0869654, acc 0.96875\n",
      "2019-05-01T11:19:17.391871: step 881, loss 0.0449462, acc 1\n",
      "2019-05-01T11:19:17.935660: step 882, loss 0.133996, acc 0.96875\n",
      "2019-05-01T11:19:18.479565: step 883, loss 0.056074, acc 0.96875\n",
      "2019-05-01T11:19:18.998119: step 884, loss 0.0675651, acc 1\n",
      "2019-05-01T11:19:19.602067: step 885, loss 0.07153, acc 1\n",
      "2019-05-01T11:19:20.053927: step 886, loss 0.0512439, acc 1\n",
      "2019-05-01T11:19:20.640514: step 887, loss 0.106257, acc 0.96875\n",
      "2019-05-01T11:19:20.791318: step 888, loss 0.127456, acc 1\n",
      "2019-05-01T11:19:21.429308: step 889, loss 0.0721132, acc 1\n",
      "2019-05-01T11:19:21.992293: step 890, loss 0.0268969, acc 1\n",
      "2019-05-01T11:19:22.550216: step 891, loss 0.100068, acc 1\n",
      "2019-05-01T11:19:23.148413: step 892, loss 0.0669198, acc 1\n",
      "2019-05-01T11:19:23.768967: step 893, loss 0.0998351, acc 0.96875\n",
      "2019-05-01T11:19:24.365560: step 894, loss 0.111502, acc 0.96875\n",
      "2019-05-01T11:19:24.861401: step 895, loss 0.0910991, acc 1\n",
      "2019-05-01T11:19:25.420210: step 896, loss 0.092792, acc 1\n",
      "2019-05-01T11:19:25.916958: step 897, loss 0.0587935, acc 1\n",
      "2019-05-01T11:19:26.523699: step 898, loss 0.0688638, acc 1\n",
      "2019-05-01T11:19:27.030961: step 899, loss 0.0779202, acc 1\n",
      "2019-05-01T11:19:27.594697: step 900, loss 0.0734672, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-01T11:19:28.481166: step 900, loss 2.46371, acc 0.12973\n",
      "\n",
      "Saved model checkpoint to /home/bear/personality_prediction/runs/1556726532/checkpoints/model-900\n",
      "\n",
      "2019-05-01T11:19:29.157224: step 901, loss 0.0714716, acc 1\n",
      "2019-05-01T11:19:29.727450: step 902, loss 0.0563812, acc 1\n",
      "2019-05-01T11:19:30.375538: step 903, loss 0.0850719, acc 1\n",
      "2019-05-01T11:19:30.860662: step 904, loss 0.0618888, acc 1\n",
      "2019-05-01T11:19:31.381223: step 905, loss 0.093441, acc 1\n",
      "2019-05-01T11:19:31.921021: step 906, loss 0.0619288, acc 1\n",
      "2019-05-01T11:19:32.517821: step 907, loss 0.0810395, acc 1\n",
      "2019-05-01T11:19:33.189150: step 908, loss 0.106741, acc 0.96875\n",
      "2019-05-01T11:19:33.830343: step 909, loss 0.0621019, acc 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8816dba6d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8816dba6d35>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     95\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     96\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36TF1.3/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36TF1.3/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36TF1.3/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36TF1.3/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py36TF1.3/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if (FLAGS.trained_embedding):\n",
    "            vocabulary = vocab_processor.vocabulary_\n",
    "            initW = None\n",
    "            print(\"Load embedding file--------------------------------------------------\")\n",
    "            #yes_transfer, no_transfer = 'all', 'labeled'\n",
    "            #l_embfile = no_transfer + '.' + str(FLAGS.embedding_dim) + '.vec'\n",
    "            al_embfile = 'all.' + str(FLAGS.embedding_dim) + '.vec'\n",
    "            if FLAGS.train_vec:\n",
    "                datafolders = [FLAGS.labeled_data_dir, FLAGS.extra_data_dir]\n",
    "                if os.path.isfile(FLAGS.embedding_dir + al_embfile):\n",
    "                    print(al_embfile + 'has already been exist')\n",
    "                else:\n",
    "                    data_helpers.train_word_embedding(FLAGS.embedding_dim, \"all\", df)\n",
    "                embfile = FLAGS.embedding_dir + al_embfile\n",
    "            else:\n",
    "                embfile = \"/home/bear/Downloads/glove.42B.300d.txt\"\n",
    "            initW = data_helpers.load_embedding_vectors(vocabulary, embfile, FLAGS.embedding_dim)\n",
    "            print('embedding file: ' + embfile + ' has been sucessfully loaded!\\n')\n",
    "            sess.run(cnn.W.assign(initW))\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(count_fre.keys(),count_fre.values(), label=\"Image number frequency distribution\",\n",
    "       color='b')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of images in unique drug')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
